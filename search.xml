<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SpringBoot整合SpringBatch]]></title>
    <url>%2F2018%2F08%2F31%2FSpringBoot%E6%95%B4%E5%90%88SpringBatch%2F</url>
    <content type="text"><![CDATA[SpringBatch简介SpringBatch是一个轻量级的综合性批处理框架,可用于开发企业信息系统中那些至关重要的数据批量处理业务. Spring Batch基于POJO和Spring框架,相当容易上手使用,让开发者很容易地访问和利用企业级服务.Spring Batch不是调度(scheduling)框架.因为已经有很多非常好的企业级调度框架,包括商业性质的和开源的,例如Quartz, Tivoli, Control-M等.它是为了与调度程序一起协作完成任务而设计的,而不是用来取代调度框架的. SpringBatch提供了大量的,可重用的功能，这些功能对大数据处理来说是必不可少的，包括 日志/跟踪(tracing)，事务管理，任务处理(processing)统计，任务重启， 忽略(skip)，和资源管理等功能。 此外还提供了许多高级服务和特性,使之能够通过优化(optimization ) 和分片技术(partitioningtechniques)来高效地执行超大型数据集的批处理任务。 SpringBatch是一个具有高可扩展性的框架,简单的批处理,或者复杂的大数据批处理作业都可以通过Spring Batch框架来实现。 SpringBoot整合SpringBatch pom文件的springbatch依赖，同时加如mysql的依赖： 1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-batch&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 在resource文件夹中新建一个csv文件，用来保存所需要进行批处理的数据： 在数据库里新建一张person表： 1234567CREATE TABLE person(id int PRIMARY KEY AUTO_INCREMENT,name VARCHAR(20),age int,nation VARCHAR(20),address VARCHAR(20)); application.yml文件指定数据库： 123456spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/basedb username: root password: root 新建Person实体类，这里不说了，直接说springbatch配置类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143@Configuration@EnableBatchProcessingpublic class CsvBatchConfig &#123; /** * ItemReader定义,用来读取数据 * 1，使用FlatFileItemReader读取文件 * 2，使用FlatFileItemReader的setResource方法设置csv文件的路径 * 3，对此对cvs文件的数据和领域模型类做对应映射 * @return * @throws Exception */ @Bean public ItemReader&lt;Person&gt; reader()throws Exception &#123; FlatFileItemReader&lt;Person&gt; reader = new FlatFileItemReader&lt;&gt;(); reader.setResource(new ClassPathResource(&quot;person.csv&quot;)); reader.setLineMapper(new DefaultLineMapper&lt;Person&gt;()&#123; &#123; setLineTokenizer(new DelimitedLineTokenizer()&#123; &#123; setNames(new String[]&#123;&quot;name&quot;,&quot;age&quot;,&quot;nation&quot;,&quot;address&quot;&#125;); &#125; &#125;); setFieldSetMapper(new BeanWrapperFieldSetMapper&lt;Person&gt;()&#123;&#123; setTargetType(Person.class); &#125;&#125;); &#125; &#125;); return reader; &#125; /** * ItemProcessor定义，用来处理数据 * @return */ @Bean public ItemProcessor&lt;Person,Person&gt; processor()&#123; //使用我们自定义的ItemProcessor的实现CsvItemProcessor CsvItemProcessor processor = new CsvItemProcessor(); //为processor指定校验器为CsvBeanValidator() processor.setValidator(csvBeanValidator()); return processor; &#125; /** * ItemWriter定义，用来输出数据 * spring能让容器中已有的Bean以参数的形式注入，Spring Boot已经为我们定义了dataSource * @param dataSource * @return */ @Bean public ItemWriter&lt;Person&gt; writer(@Qualifier(&quot;dataSource&quot;) DataSource dataSource)&#123; JdbcBatchItemWriter&lt;Person&gt; writer = new JdbcBatchItemWriter&lt;&gt;(); //我们使用JDBC批处理的JdbcBatchItemWriter来写数据到数据库 writer.setItemSqlParameterSourceProvider(new BeanPropertyItemSqlParameterSourceProvider&lt;&gt;()); String sql = &quot;insert into person &quot;+&quot; (name,age,nation,address) &quot; +&quot; values(:name,:age,:nation,:address)&quot;; //在此设置要执行批处理的SQL语句 writer.setSql(sql); writer.setDataSource(dataSource); return writer; &#125; /** * JobRepository，用来注册Job的容器 * jobRepositor的定义需要dataSource和transactionManager，Spring Boot已为我们自动配置了 * 这两个类，Spring可通过方法注入已有的Bean * @param dataSource * @param transactionManager * @return * @throws Exception */ @Bean public JobRepository jobRepository(@Qualifier(&quot;dataSource&quot;) DataSource dataSource, PlatformTransactionManager transactionManager)throws Exception&#123; JobRepositoryFactoryBean jobRepositoryFactoryBean = new JobRepositoryFactoryBean(); jobRepositoryFactoryBean.setDataSource(dataSource); jobRepositoryFactoryBean.setTransactionManager(transactionManager); jobRepositoryFactoryBean.setDatabaseType(DatabaseType.MYSQL.name()); return jobRepositoryFactoryBean.getObject(); &#125; /** * JobLauncher定义，用来启动Job的接口 * @param dataSource * @param transactionManager * @return * @throws Exception */ @Bean public SimpleJobLauncher jobLauncher(@Qualifier(&quot;dataSource&quot;) DataSource dataSource, PlatformTransactionManager transactionManager)throws Exception&#123; SimpleJobLauncher jobLauncher = new SimpleJobLauncher(); jobLauncher.setJobRepository(jobRepository(dataSource, transactionManager)); return jobLauncher; &#125; /** * Job定义，我们要实际执行的任务，包含一个或多个Step * @param jobBuilderFactory * @param s1 * @return */ @Bean public Job importJob(JobBuilderFactory jobBuilderFactory, Step s1)&#123; return jobBuilderFactory.get(&quot;importJob&quot;) .incrementer(new RunIdIncrementer()) .flow(s1)//为Job指定Step .end() .listener(csvJobListener())//绑定监听器csvJobListener .build(); &#125; /** *step步骤，包含ItemReader，ItemProcessor和ItemWriter * @param stepBuilderFactory * @param reader * @param writer * @param processor * @return */ @Bean public Step step1(StepBuilderFactory stepBuilderFactory, ItemReader&lt;Person&gt; reader, ItemWriter&lt;Person&gt; writer, ItemProcessor&lt;Person,Person&gt; processor)&#123; return stepBuilderFactory .get(&quot;step1&quot;) .&lt;Person,Person&gt;chunk(65000)//批处理每次提交65000条数据 .reader(reader)//给step绑定reader .processor(processor)//给step绑定processor .writer(writer)//给step绑定writer .build(); &#125; @Bean public CsvJobListener csvJobListener()&#123; return new CsvJobListener(); &#125; @Bean public Validator&lt;Person&gt; csvBeanValidator()&#123; return new CsvBeanValidator&lt;Person&gt;(); &#125;&#125; 自定义一个校验器： 123456789101112131415161718public class CsvItemProcessor extends ValidatingItemProcessor&lt;Person&gt; &#123; @Override public Person process(Person item) throws ValidationException &#123; /** * 需要执行super.process(item)才会调用自定义校验器 */ super.process(item); /** * 对数据进行简单的处理，若民族为汉族，则数据转换为01，其余转换为02 */ if (item.getNation().equals(&quot;汉族&quot;)) &#123; item.setNation(&quot;01&quot;); &#125; else &#123; item.setNation(&quot;02&quot;); &#125; return item; &#125;&#125; 还有数据校验类： 12345678910111213141516171819202122232425262728293031public class CsvBeanValidator&lt;T&gt; implements Validator&lt;T&gt;,InitializingBean&#123; private javax.validation.Validator validator; @Override public void validate(T value) throws ValidationException &#123; /** * 使用Validator的validate方法校验数据 */ Set&lt;ConstraintViolation&lt;T&gt;&gt; constraintViolations = validator.validate(value); if (constraintViolations.size() &gt; 0) &#123; StringBuilder message = new StringBuilder(); for (ConstraintViolation&lt;T&gt; constraintViolation : constraintViolations) &#123; message.append(constraintViolation.getMessage() + &quot;\n&quot;); &#125; throw new ValidationException(message.toString()); &#125; &#125; /** * 使用JSR-303的Validator来校验我们的数据，在此进行JSR-303的Validator的初始化 * @throws Exception */ @Override public void afterPropertiesSet() throws Exception &#123; ValidatorFactory validatorFactory = Validation.buildDefaultValidatorFactory(); validator = validatorFactory.usingContext().getValidator(); &#125;&#125; github欢迎你]]></content>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot之日志AOP]]></title>
    <url>%2F2018%2F08%2F31%2FSpringBoot%E4%B9%8B%E6%97%A5%E5%BF%97AOP%2F</url>
    <content type="text"><![CDATA[SpringBoot有自己的日志框架，比如 SLF4J和logback，今天不使用他们两个。今天就结合Spring框架学习的AOP搞一个自定义日志记录。 首先加入AOP依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 定义一个注解@： 123456789import java.lang.annotation.*;@Target(&#123;ElementType.PARAMETER, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface SysLog &#123; /** 要执行的操作类型比如：add操作 **/ String value() default &quot;&quot;;&#125; 编写一个Aspect类，使用@sysLog在Controller层的每一个方法前注入，然后根据我们的日志需求编写相应的代码，并可以通过Service把日志保存在数据库中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Aspect@Componentpublic class SysLogAspect &#123; private static final Logger logger = LoggerFactory.getLogger(SysLogAspect.class); @Autowired private LogService logService; //Controller层切点 @Pointcut(&quot;execution (* com.***.***.controller..*.*(..))&quot;) public void controllerAspect() &#123;&#125; @After(&quot;@annotation(sysLog)&quot;) public void doBefore(JoinPoint joinPoint,SysLog sysLog) &#123; String id = StringUtils.getUUID(); //获取注解里的值 String operation = sysLog.value(); HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); HttpSession session = request.getSession(); String loginName = (String) session.getAttribute(&quot;loginName&quot;); String ip = request.getRemoteHost(); String url = request.getRequestURL().toString(); String method = request.getMethod(); String targetName = joinPoint.getTarget().getClass().getName(); String methodName = joinPoint.getSignature().getName(); String date = DateUtils.getYearDate(); Log log = new Log(); log.setId(id); log.setLoginname(loginName);//用户名 log.setIp(ip);//请求ip log.setUrl(url);//请求url log.setMethod(method);//请求方式 log.setOperation(operation);//执行的操作 log.setTargetName(targetName);//请求的Controller log.setMethodName(methodName);//请求的方法 log.setDate(date);//请求时间 int result = logService.insert(log); if(result == 1 )&#123; logger.info(&quot;新增一条日志记录：&quot;+log.toString()); &#125;else&#123; logger.error(&quot;新增日志记录失败！！！&quot;); &#125; &#125;&#125; @SysLog注解方法： 效果：]]></content>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyCat 知识]]></title>
    <url>%2F2018%2F08%2F31%2FMyCat-%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Mycat是什么MyCAT是mysql中间件，前身是阿里大名鼎鼎的Cobar，Cobar在开源了一段时间后，不了了之，于是MyCAT扛起了这面大旗，在大数据时代，其重要性愈发彰显。 为什么使用MyCat为了满足数据库数据大量存储，提高查询性能，将一个数据库的数据分散到不同的数据库中存储，为应对此问题就出现了——MyCat 支持的数据库mysql，sqlserver，mongoDB等 Mycat原理可以用“拦截”一词形容，它拦截了用户发送过来的SQL语句，首先对SQL语句做了一些特定的分析，如分片分析，路由分析，读写分离分析，缓存分析等，然后将此sql发往后端的真实数据库，并将返回的结果做适当处理，最终返回给用户。 分片数据库分片指：通过某种特定的条件，将我们存放在一个数据库中的数据分散存放在不同的多个数据库（主机）中，这样来达到分散单台设备的负载，根据切片规则，可分为以下两种切片模式： 垂直切片 将不同的表切分到不同的数据库中 水平切片 将同一种表按照某种条件切分到不同的数据库中（这种方式最常用） MyCAT通过定义表的分片规则来实现分片，每个表格可以捆绑一个分片规则，每个分片规则指定一个分片字段并绑定一个函数，来实现动态分片算法 Schema：逻辑库，与MySQL中的Database（数据库）对应，一个逻辑库中定义了所包括的Table。 Table：逻辑表，即物理数据库中存储的某一张表，与传统数据库不同，这里的表格需要声明其所存储的逻辑数据节点DataNode。在此可以指定表的分片规则。 DataNode：MyCAT的逻辑数据节点，是存放table的具体物理节点，也称之为分片节点，通过DataSource来关联到后端某个具体数据库上 DataSource：定义某个物理库的访问地址，用于捆绑到Datanode上配置文件详解 mycat主要有3个配置文件，rule.xml,schema.xml和server.xml 1. rule.xml（分片规则的配置文件） 该文件主要定义了分片的规则，这个文件里面主要有tableRule和function这两个标签。在具体使用过程中可以按照需求添加tableRule和function。 123456&lt;tableRule name=&quot;auto-sharding-long&quot;&gt; &lt;rule&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;algorithm&gt;rang-long&lt;/algorithm&gt; &lt;/rule&gt;&lt;/tableRule&gt; name：用户标识不同的分表规则 columns：指定按哪一列进行拆分 algorithm：该属性值为function标签中name的属性值，定义了连接表规则的具体的路由算法，多个表规则可以连接到同一个路由算法上 1234&lt;function name=&quot;rang-long&quot; class=&quot;org.opencloudb.route.function.AutoPartitionByLong&quot;&gt; &lt;property name=&quot;mapFile&quot;&gt;autopartition-long.txt&lt;/property&gt;&lt;/function&gt; name：标识算法的名字 class：指定路由算法具体的类名字 property：具体算法用到的一些属性 2. schema.xml（逻辑库定义和表以及分片定义的配置文件）该文件是MyCat中重要的配置文件之一，管理着MyCat的逻辑库、表、分片规则、DataNode以及DataSource. 12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt; &lt;table name=&quot;travelrecord&quot; dataNode=&quot;dn1,dn2,dn3&quot; &lt;table name=&quot;company&quot; primaryKey=&quot;ID&quot; type=&quot;global&quot; dataNode=&quot;dn1,dn2,dn3&quot; /&gt; &lt;table name=&quot;goods&quot; primaryKey=&quot;ID&quot; type=&quot;global&quot; dataNode=&quot;dn1,dn2&quot; /&gt; &lt;/schema&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;db1&quot; /&gt; &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;localhost1&quot; database=&quot;db2&quot; /&gt; &lt;dataNode name=&quot;dn3&quot; dataHost=&quot;localhost1&quot; database=&quot;db3&quot; /&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot; maxRetryCount=&quot;4&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;hostM1&quot; url=&quot;192.168.0.184:3306&quot; user=&quot;root&quot; password=&quot;123456&quot;&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; schema标签：定义mycat实例中的逻辑库 table标签：定义mycat实例中的逻辑表 dataNode标签：定义mycat中的数据节点，也是通常说的数据分片 dataHost标签：作为最底层标签存在，定义了具体的真正存放数据的数据库实例，读写分离配置和心跳语句，这里只配了一个dataHost，如果你配了N个主机，就要配N个dataHost节点 3. server.xml（Mycat服务器参数调整和用户授权的配置文件）12345678910&lt;!DOCTYPE mycat:server SYSTEM &quot;server.dtd&quot;&gt;&lt;mycat:server xmlns:mycat=&quot;http://org.opencloudb/&quot;&gt; &lt;system&gt; &lt;property name=&quot;nonePasswordLogin&quot;&gt;0&lt;/property&gt; &lt;!-- 0为需要密码登陆、1为不需要密码登陆 ,默认为0，设置为1则需要指定默认账户--&gt; &lt;/system&gt; &lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt; &lt;property name=&quot;password&quot;&gt;123456&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt; &lt;/user&gt;&lt;/mycat:server&gt; system标签：内嵌的所有property标签都与系统配置有关 user标签 name：逻辑用户名，即登录mycat的用户名 password：逻辑密码，即登录mycat的用户名对应的密码 schemas：逻辑数据库，可配置多个，用英文逗号隔开，对应于schema.xml文件中配置的逻辑数据库，两者对应 readOnly：该数据库是否为只读，如果true就是只读使用Mycat 把MyCat的压缩包上传到linux服务器 解压缩 tar zxf 压缩包名称 进入mycat目录，创建logos/mycat.pid 123mkdir logscd logstouch mycat.pid 进入mycat/bin目录 123启动命令：./mycat start 停止命令：./mycat stop 重启命令：./mycat restart mycat连接mysql数据库 1mysql -uroot -p123456 -h192.168.0.184 -P8066 -DTESTDB 8066是mycat的监听端口，类似于mysql的3306端口，其中-u，-p，-h分别是用户名，密码和主机，-D是连接的逻辑库。 创建表 1create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int); 插入数据 12345678mysql&gt; insert into travelrecord(id,user_id,traveldate,fee,days) values(1,&apos;Victor&apos;,20160101,100,10);Query OK, 1 row affected (0.58 sec)mysql&gt; insert into travelrecord(id,user_id,traveldate,fee,days) values(5000001,&apos;Job&apos;,20160102,100,10);Query OK, 1 row affected (0.01 sec)mysql&gt; insert into travelrecord(id,user_id,traveldate,fee,days) values(10000001,&apos;Slow&apos;,20160103,100,10);Query OK, 1 row affected (0.01 sec) 查看分片结果 1234567891011121314151617181920212223mysql&gt; select * from db1.travelrecord;+----+---------+------------+------+------+| id | user_id | traveldate | fee | days |+----+---------+------------+------+------+| 1 | Victor | 2016-01-01 | 100 | 10 |+----+---------+------------+------+------+1 row in set (0.00 sec)mysql&gt; select * from db2.travelrecord;+---------+---------+------------+------+------+| id | user_id | traveldate | fee | days |+---------+---------+------------+------+------+| 5000001 | Job | 2016-01-02 | 100 | 10 |+---------+---------+------------+------+------+1 row in set (0.00 sec)mysql&gt; select * from db3.travelrecord;+----------+---------+------------+------+------+| id | user_id | traveldate | fee | days |+----------+---------+------------+------+------+| 10000001 | Slow | 2016-01-03 | 100 | 10 |+----------+---------+------------+------+------+1 row in set (0.00 sec) explain命令查看create语句和insert语句具体会分配到哪些Datanode上 123456789101112131415161718192021222324252627282930313233mysql&gt; explain create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int);+-----------+-----------------------------------------------------------------------------------------------------------------------+| DATA_NODE | SQL |+-----------+-----------------------------------------------------------------------------------------------------------------------+| dn1 | create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int) || dn2 | create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int) || dn3 | create table travelrecord (id bigint not null primary key,user_id varchar(100),traveldate DATE, fee decimal,days int) |+-----------+-----------------------------------------------------------------------------------------------------------------------+3 rows in set (0.34 sec)mysql&gt; explain insert into travelrecord(id,user_id,traveldate,fee,days) values(1,&apos;Victor&apos;,20160101,100,10);+-----------+----------------------------------------------------------------------------------------------+| DATA_NODE | SQL |+-----------+----------------------------------------------------------------------------------------------+| dn1 | insert into travelrecord(id,user_id,traveldate,fee,days) values(1,&apos;Victor&apos;,20160101,100,10) |+-----------+----------------------------------------------------------------------------------------------+1 row in set (0.05 sec)mysql&gt; explain insert into travelrecord(id,user_id,traveldate,fee,days) values(5000001,&apos;Job&apos;,20160102,100,10);+-----------+-------------------------------------------------------------------------------------------------+| DATA_NODE | SQL |+-----------+-------------------------------------------------------------------------------------------------+| dn2 | insert into travelrecord(id,user_id,traveldate,fee,days) values(5000001,&apos;Job&apos;,20160102,100,10) |+-----------+-------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)mysql&gt; explain insert into travelrecord(id,user_id,traveldate,fee,days) values(10000001,&apos;Slow&apos;,20160103,100,10);+-----------+---------------------------------------------------------------------------------------------------+| DATA_NODE | SQL |+-----------+---------------------------------------------------------------------------------------------------+| dn3 | insert into travelrecord(id,user_id,traveldate,fee,days) values(10000001,&apos;Slow&apos;,20160103,100,10) |+-----------+---------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)]]></content>
      <categories>
        <category>Mycat</category>
      </categories>
      <tags>
        <tag>MyCat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot（三）：文件下载]]></title>
    <url>%2F2018%2F08%2F30%2FSpringBoot%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[在原来的SpringBoot–uploadfile项目基础上添加文件下载的Controller： 1234567891011121314151617181920212223242526272829303132 @RequestMapping(value = &quot;/testDownload&quot;, method = RequestMethod.GET)public void Download(HttpServletResponse res) &#123; String fileName = &quot;1.png&quot;; res.setHeader(&quot;content-type&quot;, &quot;application/octet-stream&quot;); res.setContentType(&quot;application/octet-stream&quot;); res.setHeader(&quot;Content-Disposition&quot;, &quot;attachment;filename=&quot; + fileName); byte[] buff = new byte[1024]; BufferedInputStream bis = null; OutputStream os = null; try &#123; os = res.getOutputStream(); bis = new BufferedInputStream(new FileInputStream(new File(&quot;d://&quot; + fileName))); int i = bis.read(buff); while (i != -1) &#123; os.write(buff, 0, buff.length); os.flush(); i = bis.read(buff); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (bis != null) &#123; try &#123; bis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; System.out.println(&quot;success&quot;); &#125; 需要下载的文件放在D盘。 1234 @RequestMapping(value = &quot;/download&quot;, method = RequestMethod.GET)public String Download() &#123; return &quot;/fileDownload&quot;;&#125; fileDownload.html： 1234567891011&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;UTF-8&quot;/&gt;&lt;title&gt;文件下载示例&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;文件下载示例&lt;/h2&gt; &lt;hr/&gt; &lt;a href=&quot;/testDownload&quot;&gt;下载&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot（一）：HelloWorld]]></title>
    <url>%2F2018%2F08%2F30%2FSpringBoot%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AHelloWorld%2F</url>
    <content type="text"><![CDATA[什么是spring boot Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。用我的话来理解，就是spring boot其实不是什么新的框架，它默认配置了很多框架的使用方式，就像maven整合了所有的jar包，spring boot整合了所有的框架（不知道这样比喻是否合适）。 使用spring boot有什么好处 其实就是简单、快速、方便！平时如果我们需要搭建一个spring web项目的时候需要怎么做呢？ 1）配置web.xml，加载spring和spring mvc 2）配置数据库连接、配置spring事务 3）配置加载配置文件的读取，开启注解 4）配置日志文件 … maven构建项目 1、访问http://start.spring.io/ 2、选择构建工具Maven Project、Spring Boot版本1.3.6以及一些工程基本信息，点击“Switch to the full version.”java版本选择1.7，可参考下图所示： 3、点击Generate Project下载项目压缩包 4、解压后，使用eclipse，Import -&gt; Existing Maven Projects -&gt; Next -&gt;选择解压后的文件夹-&gt; Finsh，OK done! 项目结构介绍 如上图所示，Spring Boot的基础结构共三个文件: l src/main/java 程序开发以及主程序入口 l src/main/resources 配置文件 l src/test/java 测试程序 另外，spingboot建议的目录结果如下： root package结构：com.example.demo 特别的： Spring Boot中的Application.java必须放在Java目录的首级！！！！ 采用默认配置可以省去很多配置，当然也可以根据自己的喜欢来进行更改 最后，启动Application main方法，至此一个java项目搭建好了！ 引入web模块 1、pom.xml中添加支持web的模块： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; pom.xml文件中默认有两个模块： spring-boot-starter：核心模块，包括自动配置支持、日志和YAML； spring-boot-starter-test：测试模块，包括JUnit、Hamcrest、Mockito。 2、编写controller内容 1234567@RestControllerpublic class HelloWorldController &#123; @RequestMapping(&quot;/hello&quot;) public String index() &#123; return &quot;Hello World&quot;; &#125;&#125; @RestController的意思就是controller里面的方法都以json格式输出，不用再写什么jackjson配置的了！ 3、启动主程序，打开浏览器访问http://localhost:8080/hello，就可以看到效果了，有木有很简单！ 如何做单元测试 打开的src/test/下的测试入口，编写简单的http请求来测试；使用mockmvc进行，利用MockMvcResultHandlers.print()打印出执行结果。 1234567891011121314151617@RunWith(SpringJUnit4ClassRunner.class)@SpringApplicationConfiguration(classes = MockServletContext.class)@WebAppConfigurationpublic class HelloWorldControlerTests &#123; private MockMvc mvc; @Before public void setUp() throws Exception &#123; mvc = MockMvcBuilders.standaloneSetup(new HelloWorldController()).build(); &#125; @Test public void getHello() throws Exception &#123; mvc.perform(MockMvcRequestBuilders.get(&quot;/hello&quot;).accept(MediaType.APPLICATION_JSON)) .andExpect(MockMvcResultMatchers.status().isOk()) .andDo(MockMvcResultHandlers.print()) .andReturn(); &#125;&#125; 开发环境的调试 热启动在正常开发项目中已经很常见了吧，虽然平时开发web项目过程中，改动项目启重启总是报错；但springBoot对调试支持很好，修改之后可以实时生效，需要添加以下的配置： 1234567891011121314151617181920212223242526272829&lt;dependencies&gt; *&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;*&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; *&lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;/configuration&gt;* &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 该模块在完整的打包环境下运行的时候会被禁用。如果你使用java -jar启动应用或者用一个特定的classloader启动，它会认为这是一个“生产环境”。 总结 使用spring boot可以非常方便、快速搭建项目，使我们不用关心框架之间的兼容性，适用版本等各种问题，我们想使用任何东西，仅仅添加一个配置就可以，所以使用sping boot非常适合构建微服务。]]></content>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot （二）：文件上传]]></title>
    <url>%2F2018%2F08%2F30%2FSpringBoot-%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[新建一个SpringBoot项目：SpringBoot–uploadfile工程目录： pom.xml依赖： 123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- thmleaf模板依赖. --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; FileUploadConfiguration文件信息配置： 12345678910111213141516public class FileUploadConfiguration &#123; @Bean public MultipartConfigElement multipartConfigElement() &#123; MultipartConfigFactory factory = new MultipartConfigFactory(); // 设置文件大小限制 ,超出设置页面会抛出异常信息， // 这样在文件上传的地方就需要进行异常信息的处理了; factory.setMaxFileSize(&quot;256KB&quot;); // KB,MB /// 设置总上传数据总大小 factory.setMaxRequestSize(&quot;512KB&quot;); // Sets the directory location where files will be stored. // factory.setLocation(&quot;路径地址&quot;); return factory.createMultipartConfig(); &#125;&#125; Controller类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Controllerpublic class FileUploadController &#123; @RequestMapping(value = &quot;/upload&quot;, method = RequestMethod.GET) public String upload() &#123; return &quot;/fileupload&quot;; &#125; @RequestMapping(value = &quot;/upload/batch&quot;, method = RequestMethod.GET) public String batchUpload() &#123; return &quot;/mutifileupload&quot;; &#125; @RequestMapping(value = &quot;/upload&quot;, method = RequestMethod.POST) @ResponseBody public String upload(@RequestParam(&quot;file&quot;) MultipartFile file) &#123; if (!file.isEmpty()) &#123; try &#123; // 这里只是简单例子，文件直接输出到项目路径下。 // 实际项目中，文件需要输出到指定位置，需要在增加代码处理。 // 还有关于文件格式限制、文件大小限制，详见：中配置。 BufferedOutputStream out = new BufferedOutputStream( new FileOutputStream(new File(file.getOriginalFilename()))); out.write(file.getBytes()); out.flush(); out.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); return &quot;上传失败,&quot; + e.getMessage(); &#125; catch (IOException e) &#123; e.printStackTrace(); return &quot;上传失败,&quot; + e.getMessage(); &#125; return &quot;上传成功&quot;; &#125; else &#123; return &quot;上传失败，因为文件是空的.&quot;; &#125; &#125; /** * 多文件上传 主要是使用了MultipartHttpServletRequest和MultipartFile * */ @RequestMapping(value = &quot;/upload/batch&quot;, method = RequestMethod.POST) public @ResponseBody String batchUpload(HttpServletRequest request) &#123; List&lt;MultipartFile&gt; files = ((MultipartHttpServletRequest) request).getFiles(&quot;file&quot;); MultipartFile file = null; BufferedOutputStream stream = null; for (int i = 0; i &lt; files.size(); ++i) &#123; file = files.get(i); if (!file.isEmpty()) &#123; try &#123; byte[] bytes = file.getBytes(); stream = new BufferedOutputStream(new FileOutputStream(new File(file.getOriginalFilename()))); stream.write(bytes); stream.close(); &#125; catch (Exception e) &#123; stream = null; return &quot;You failed to upload &quot; + i + &quot; =&gt; &quot; + e.getMessage(); &#125; &#125; else &#123; return &quot;You failed to upload &quot; + i + &quot; because the file was empty.&quot;; &#125; &#125; return &quot;upload successful&quot;; &#125; &#125; HTML文件配置：单文件： 1234567891011121314&lt;title&gt;文件上传示例&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;文件上传示例&lt;/h2&gt; &lt;hr/&gt; &lt;form method=&quot;POST&quot; enctype=&quot;multipart/form-data&quot; action=&quot;/upload&quot;&gt; &lt;p&gt; 文件：&lt;input type=&quot;file&quot; name=&quot;file&quot; /&gt; &lt;/p&gt; &lt;p&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot; /&gt; &lt;/p&gt; &lt;/form&gt;&lt;/body&gt; 批量文件： 12345678910111213141516171819202122&lt;title&gt;批量文件上传示例&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;批量文件上传示例&lt;/h2&gt; &lt;hr/&gt; &lt;form method=&quot;POST&quot; enctype=&quot;multipart/form-data&quot; action=&quot;/upload/batch&quot;&gt; &lt;p&gt; 文件1：&lt;input type=&quot;file&quot; name=&quot;file&quot; /&gt; &lt;/p&gt; &lt;p&gt; 文件2：&lt;input type=&quot;file&quot; name=&quot;file&quot; /&gt; &lt;/p&gt; &lt;p&gt; 文件3：&lt;input type=&quot;file&quot; name=&quot;file&quot; /&gt; &lt;/p&gt; &lt;p&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot; /&gt; &lt;/p&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成Solr]]></title>
    <url>%2F2018%2F08%2F30%2FSpringBoot%E9%9B%86%E6%88%90Solr-1%2F</url>
    <content type="text"><![CDATA[添加springboot集成solr的依赖 1234 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-solr&lt;/artifactId&gt;&lt;/dependency&gt; application.yml指向solr地址 1234spring: data: solr: host: http://192.168.0.197:8983/solr/ik_core 添加solr实时更新操作 123456789101112131415161718192021222324252627282930313233343536373839404142@SpringBootApplication@EnableSchedulingpublic class SpringbootSolrApplication &#123; private Logger logger = LoggerFactory.getLogger(this.getClass()); @Autowired private RestTemplateBuilder builder; @Autowired private RestTemplate restTemplate; // 使用RestTemplateBuilder来实例化RestTemplate对象，spring默认已经注入了RestTemplateBuilder实例 @Bean public RestTemplate restTemplate() &#123; return builder.build(); &#125; public static void main(String[] args) &#123; SpringApplication.run(SpringbootSolrApplication.class, args); &#125; //每五秒执行一次 @Scheduled(cron = &quot;0/5 * * * * *&quot;) public void updateSolr() &#123; MultiValueMap&lt;String, Object&gt; postParameters = new LinkedMultiValueMap&lt;&gt;(); postParameters.add(&quot;command&quot;, &quot;full-import&quot;); postParameters.add(&quot;verbose&quot;, &quot;false&quot;); postParameters.add(&quot;clean&quot;, &quot;true&quot;); postParameters.add(&quot;commit&quot;, &quot;true&quot;); postParameters.add(&quot;core&quot;, &quot;ik_core&quot;); postParameters.add(&quot;name&quot;, &quot;dataimport&quot;); HttpHeaders headers = new HttpHeaders(); headers.add(&quot;Content-Type&quot;, &quot;application/x-www-form-urlencoded&quot;); HttpEntity &lt;MultiValueMap &lt;String, Object&gt;&gt; r = new HttpEntity&lt;&gt;(postParameters, headers); String time = String.valueOf(new Date().getTime()); String url = &quot;http://192.168.0.197:8983/solr/ik_core/dataimport?_=&quot; + time + &quot;&amp;indent=on&amp;wt=json&quot;; String responseMessage = restTemplate.postForObject(url, r, String.class); logger.info(&quot;更新solr索引：返回值：&#123;&#125;&quot;, responseMessage); &#125;&#125; 测试查询（增加了高亮显示） 123456789101112131415161718192021222324252627282930313233343536373839404142@Test public void test() throws IOException, SolrServerException &#123; SolrQuery solrQuery = new SolrQuery(); solrQuery.set(&quot;q&quot;, &quot;title:*&quot;); solrQuery.set(&quot;start&quot;, 0); solrQuery.set(&quot;rows&quot;, 20); //======高亮设置=== //开启高亮 solrQuery.setHighlight(true); //高亮域 solrQuery.addHighlightField(&quot;title&quot;); //前缀 solrQuery.setHighlightSimplePre(&quot;&lt;span style=&apos;color:red&apos;&gt;&quot;); //后缀 solrQuery.setHighlightSimplePost(&quot;&lt;/span&gt;&quot;); QueryResponse response = solrClient.query(solrQuery); SolrDocumentList results = response.getResults(); System.out.println(&quot;查询内容:&quot; + solrQuery); System.out.println(&quot;文档数量：&quot; + results.getNumFound()); System.out.println(&quot;查询花费时间:&quot; + response.getQTime()); //获取高亮信息 Map&lt;String, Map&lt;String, List&lt;String&gt;&gt;&gt; highlighting = response.getHighlighting(); for (SolrDocument solrDocument :results) &#123; System.out.println(solrDocument); System.out.println(solrDocument.getFieldValue(&quot;cover&quot;)); System.out.println(solrDocument.getFieldValue(&quot;service_area&quot;)); //输出高亮 Map&lt;String, List&lt;String&gt;&gt; map = highlighting.get(solrDocument.get(&quot;id&quot;)); List&lt;String&gt; list = map.get(&quot;title&quot;); if(list != null &amp;&amp; list.size() &gt; 0)&#123; System.out.println(list.get(0)); &#125; &#125; &#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker+Solr+IK]]></title>
    <url>%2F2018%2F08%2F30%2FDocker-Solr-IK%2F</url>
    <content type="text"><![CDATA[docker安装solr 1docker pull solr 启动solr镜像 1docker run -d -p 8983:8983 --name mysolr solr 123451. run 运行容器2. -d 代表后台运行3. -p 容器端口和宿机端口映射4. --name 容器名称5. solr 镜像名称 新建core 1docker exec -it --user=solr mysolr bin/solr create_core -c ik_core 进入solr容器 1docker exec -it -u root mysolr /bin/bash 安装vim(编辑容器里的文件) 12apt-get updateapt-get install vim 安装rzsz(上传下载容器里的文件) 1apt-get install lrzsz 进入/opt/solr/server/solr-webapp/webapp/WEB-INF/lib添加jar包 1234ik-analyzer-7.4.0.jarmysql-connector-java-8.0.11.jarsolr-dataimporthandler-7.4.0.jarsolr-dataimporthandler-extras-7.4.0.jar 进入/opt/solr/server/solr/ik_core/conf，新建data-config.xml 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;dataConfig&gt; &lt;dataSource name=&quot;source1&quot; type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://***.**.**.**:3306/*****&quot; user=&quot;***&quot; password=&quot;*****&quot; batchSize=&quot;-1&quot; /&gt; &lt;document&gt; &lt;entity name=&quot;service_info&quot; dataSource=&quot;source1&quot; query=&quot;SELECT id,title,business_type_id,category_id , service_price , service_time,service_desc,service_area, cover,click_count,create_by,create_date,del_flag FROM service_info where del_flag = 1&quot;&gt; &lt;field column=&apos;id&apos; name=&apos;id&apos; /&gt; &lt;field column=&apos;title&apos; name=&apos;title&apos; /&gt; &lt;field column=&apos;business_type_id&apos; name=&apos;business_type_id&apos; /&gt; &lt;field column=&apos;category_id&apos; name=&apos;category_id&apos; /&gt; &lt;field column=&apos;service_price&apos; name=&apos;service_price&apos; /&gt; &lt;field column=&apos;service_time&apos; name=&apos;service_time&apos; /&gt; &lt;field column=&apos;service_desc&apos; name=&apos;service_desc&apos; /&gt; &lt;field column=&apos;service_area&apos; name=&apos;service_area&apos; /&gt; &lt;field column=&apos;cover&apos; name=&apos;cover&apos; /&gt; &lt;field column=&apos;click_count&apos; name=&apos;click_count&apos; /&gt; &lt;field column=&apos;create_by&apos; name=&apos;create_by&apos; /&gt; &lt;field column=&apos;create_date&apos; name=&apos;create_date&apos; /&gt; &lt;field column=&apos;del_flag&apos; name=&apos;del_flag&apos; /&gt; &lt;/entity&gt; &lt;/document&gt;&lt;/dataConfig&gt; 配置managed-schema，加入IK分词 123456789101112131415161718192021 &lt;!-- ik分词器 --&gt;&lt;fieldType name=&quot;text_ik&quot; class=&quot;solr.TextField&quot;&gt; &lt;analyzer type=&quot;index&quot; useSmart=&quot;false&quot; class=&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot; /&gt; &lt;analyzer type=&quot;query&quot; useSmart=&quot;true&quot; class=&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot; /&gt;&lt;/fieldType&gt; &lt;field name=&apos;id&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;&lt;field name=&apos;title&apos; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot; omitNorms = &quot;false&quot; omitTermFreqAndPositions =&quot;false&quot;/&gt; &lt;field name=&apos;business_type_id&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&apos;category_id&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&apos;service_price&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&apos;service_time&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&apos;service_desc&apos; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&apos;service_area&apos; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;&lt;field name=&apos;cover&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;&lt;field name=&apos;click_count&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&apos;create_by&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; &lt;field name=&apos;create_date&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;&lt;field name=&apos;del_flag&apos; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; 配置solrconfig.xml 12345&lt;requestHandler name=&quot;/dataimport&quot; class=&quot;org.apache.solr.handler.dataimport.DataImportHandler&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;config&quot;&gt;data-config.xml&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; 重启solr容器 1docker restart 45b022b95c71]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成Memcached]]></title>
    <url>%2F2018%2F08%2F29%2FSpringBoot%E9%9B%86%E6%88%90Memcached%2F</url>
    <content type="text"><![CDATA[Docker安装Memcached 12docker pull Memcacheddocker run -name myMemcached -d -p 11211:11211 docker.io/memcached pom坐标 12345 &lt;dependency&gt; &lt;groupId&gt;com.whalin&lt;/groupId&gt; &lt;artifactId&gt;Memcached-Java-Client&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt;&lt;/dependency&gt; application.properties 123456789#缓存机制配置memcache.servers=192.168.0.197:11211memcache.weights=5memcache.initConn=20memcache.minConn=10memcache.maxConn=50memcache.maintSleep=3000memcache.nagle=falsememcache.socketTO=3000 配置SockIOPoolConfig 1234567891011121314151617181920@Data@Component@ConfigurationProperties(prefix = &quot;memcache&quot;)public class SockIOPoolConfig &#123; private String[] servers; private Integer[] weights; private int initConn; private int minConn; private int maxConn; private long maintSleep; private boolean nagle; private int socketTO;&#125; 配置MemcacheConfig，连接Memcached服务器 12345678910111213141516171819202122232425262728293031323334@Componentpublic class MemcacheConfig &#123; @Autowired SockIOPoolConfig sockIOPoolConfig; @Bean public SockIOPool sockIOPool()&#123; //获取连接池的实例 SockIOPool pool = SockIOPool.getInstance(); //服务器列表及其权重 String[] servers = sockIOPoolConfig.getServers(); Integer[] weights = sockIOPoolConfig.getWeights(); //设置服务器信息 pool.setServers(servers); pool.setWeights(weights); //设置初始连接数、最小连接数、最大连接数、最大处理时间 pool.setInitConn(sockIOPoolConfig.getInitConn()); pool.setMinConn(sockIOPoolConfig.getMinConn()); pool.setMaxConn(sockIOPoolConfig.getMaxConn()); //设置连接池守护线程的睡眠时间 pool.setMaintSleep(sockIOPoolConfig.getMaintSleep()); //设置TCP参数，连接超时 pool.setNagle(sockIOPoolConfig.isNagle()); pool.setSocketConnectTO(sockIOPoolConfig.getSocketTO()); //初始化并启动连接池 pool.initialize(); return pool; &#125; @Bean public MemCachedClient memCachedClient()&#123; return new MemCachedClient(); &#125;&#125; 测试 123456789101112131415 @AutowiredMemCachedClient memCachedClient;@Testpublic void contextLoads() &#123; boolean i = memCachedClient.set(&quot;id&quot;, &quot;123456&quot;, 1000); System.out.println(String.valueOf(i)); System.out.println(memCachedClient.get(&quot;id&quot;)); memCachedClient.replace(&quot;id&quot;,&quot;123&quot;); memCachedClient.replace(&quot;ok&quot;,&quot;123&quot;); System.out.println(memCachedClient.get(&quot;id&quot;)); memCachedClient.delete(&quot;id&quot;); System.out.println(memCachedClient.get(&quot;id&quot;)); System.out.println(memCachedClient.get(&quot;ok&quot;));&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 集成 ElasticSearch6.3.2]]></title>
    <url>%2F2018%2F08%2F29%2FSpringBoot-%E9%9B%86%E6%88%90-ElasticSearch6-3-2%2F</url>
    <content type="text"><![CDATA[SpringBoot 集成 ElasticSearch的maven坐标 1234567891011121314151617 &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;6.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;6.3.2&lt;/version&gt;&lt;/dependency&gt; 配置application.properties 12345678#elk集群地址elasticsearch.ip=192.168.0.197#端口elasticsearch.port=9300#集群名称elasticsearch.cluster.name=my-es#连接池elasticsearch.pool=5 配置ElasticSearch源 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Configurationpublic class ESconfig &#123; private static final Logger LOGGER = LoggerFactory.getLogger(ESconfig.class); /** * elk集群地址 */ @Value(&quot;$&#123;elasticsearch.ip&#125;&quot;) private String hostName; /** * 端口 */ @Value(&quot;$&#123;elasticsearch.port&#125;&quot;) private String port; /** * 集群名称 */ @Value(&quot;$&#123;elasticsearch.cluster.name&#125;&quot;) private String clusterName; /** * 连接池 */ @Value(&quot;$&#123;elasticsearch.pool&#125;&quot;) private String poolSize; @Bean public TransportClient init() &#123; LOGGER.info(&quot;初始化开始。。。。。&quot;); TransportClient transportClient = null; try &#123; // 配置信息 Settings esSetting = Settings.builder() .put(&quot;client.transport.sniff&quot;, true)//增加嗅探机制，找到ES集群 .put(&quot;thread_pool.search.size&quot;, Integer.parseInt(poolSize))//增加线程池个数，暂时设为5 .put(&quot;cluster.name&quot;,clusterName) .build(); //配置信息Settings自定义,下面设置为EMPTY transportClient = new PreBuiltTransportClient(esSetting); TransportAddress transportAddress = new TransportAddress(InetAddress.getByName(hostName), Integer.valueOf(port)); transportClient.addTransportAddresses(transportAddress); &#125; catch (Exception e) &#123; LOGGER.error(&quot;elasticsearch TransportClient create error!!!&quot;, e); &#125; return transportClient; &#125;&#125; 测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * * 功能描述: 增加数据 * */ @PostMapping(&quot;person/add&quot;) public ResponseEntity add(@RequestBody Person person) &#123; try &#123; XContentBuilder content = XContentFactory.jsonBuilder().startObject() .field(&quot;name&quot;, person.getName()) .field(&quot;age&quot;, person.getAge()) .field(&quot;work&quot;, person.getWork()) .endObject(); String id = String.valueOf(person.getId()); IndexResponse result = this.client.prepareIndex(&quot;data&quot;, &quot;person&quot;,id).setSource(content).get(); return new ResponseEntity(result.getId(), HttpStatus.OK); &#125; catch (IOException e) &#123; e.printStackTrace(); return new ResponseEntity(HttpStatus.INTERNAL_SERVER_ERROR); &#125; &#125; /** * 功能描述: 查找数据 * */ @GetMapping(&quot;person/get&quot;) public ResponseEntity get(@RequestParam(name = &quot;id&quot;, defaultValue=&quot;&quot;) String id) &#123; if (id.isEmpty())&#123; return new ResponseEntity(HttpStatus.NOT_FOUND); &#125; GetResponse result = client.prepareGet(&quot;data&quot;, &quot;person&quot;, id).get(); if (!result.isExists()) &#123; return new ResponseEntity(HttpStatus.NOT_FOUND); &#125; return new ResponseEntity(result.getSource(), HttpStatus.OK); &#125; /** * * 功能描述: 更新数据 * */ @PutMapping(&quot;person/update&quot;) public ResponseEntity update(@RequestBody Person person)&#123; try &#123; XContentBuilder builder = XContentFactory.jsonBuilder().startObject(); if (person.getName()!= null) &#123; builder.field(&quot;name&quot;, person.getName()); &#125; if (person.getAge() != null) &#123; builder.field(&quot;age&quot;, person.getAge()); &#125; if(person.getWork() != null)&#123; builder.field(&quot;work&quot;, person.getWork()); &#125; builder.endObject(); String id = String.valueOf(person.getId()); UpdateRequest updateRequest = new UpdateRequest(&quot;data&quot;, &quot;person&quot;, id); updateRequest.doc(builder); UpdateResponse result = client.update(updateRequest).get(); return new ResponseEntity(result.getResult().toString(), HttpStatus.OK); &#125; catch (Exception e) &#123; e.printStackTrace(); return new ResponseEntity(HttpStatus.INTERNAL_SERVER_ERROR); &#125; &#125; /** * * 功能描述: 删除数据 * */ @DeleteMapping(&quot;person/delete&quot;) public ResponseEntity delete(@RequestParam(name = &quot;id&quot;) String id) &#123; DeleteResponse result = client.prepareDelete(&quot;data&quot;, &quot;person&quot;, id).get(); return new ResponseEntity(result.getResult().toString(), HttpStatus.OK); &#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch6.3.2及其head插件安装]]></title>
    <url>%2F2018%2F08%2F29%2FElasticsearch6-3-2%E5%8F%8A%E5%85%B6head%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[下载并解压Elasticsearch6.3.2 进入config文件夹修改elasticsearch.yml文件 1234567891011121314151617# 集群的名字 cluster.name: my-es# 节点名字 node.name: node-1 # 数据存储目录（多个路径用逗号分隔） path.data: /home/els/es/data# 日志目录 path.logs: /home/els/es/log #本机的ip地址network.host: 192.168.0.197 # 设置节点间交互的tcp端口（集群）,(默认9300) transport.tcp.port: 9300 # 监听端口（默认） http.port: 9200 # 增加参数，使head插件可以访问es http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; 进入bin文件夹启动es ./elasticsearch 下载载并解压head插件 修改_site文件夹中的app.js 1this.base_uri = this.config.base_uri || this.prefs.get(&quot;app-base_uri&quot;) || &quot;http://192.168.0.197:9200&quot;; 安装npm 1yum install git npm 安装grunt 1npm install -g grunt-cli 启动head 1grunt server]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo github搭建博客常用的命令]]></title>
    <url>%2F2018%2F08%2F29%2Fhexo-github%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[常用命令 npm install hexo -g #安装 npm update hexo -g #升级 hexo init #初始化 hexo new “postName” #新建文章 hexo new page “pageName” #新建页面 hexo generate #生成静态页面至public目录 hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server） hexo deploy #将.deploy目录部署到GitHub hexo n #写文章 hexo g #生成 hexo d #部署 #可与hexo g合并为hexo d -g 一般操作，在博客目录下调出cmd命令窗口：1hexo new post &quot;xx&quot; 在E:\Blog\source_posts下，找到xx.md 用markdown编辑，完成后，先开启本地测试hexo s]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx负载均衡详解]]></title>
    <url>%2F2018%2F08%2F28%2FNginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[常见的负载均衡 热备：如果你有2台服务器，当一台服务器发生事故时，才启用第二台服务器给提供服务。服务器处理请求的顺序：AAAAAA突然A挂啦，BBBBBBBBBBBBBB….. 1234upstream mysvr &#123; server 127.0.0.1:7878; server 192.168.10.121:3333 backup; #热备 &#125; 轮询：nginx默认就是轮询其权重都默认为1，服务器处理请求的顺序：ABABABABAB…. 1234upstream mysvr &#123; server 127.0.0.1:7878; server 192.168.10.121:3333; &#125; 加权轮询：跟据配置的权重的大小而分发给不同服务器不同数量的请求。如果不设置，则默认为1。下面服务器的请求顺序为：ABBABBABBABBABB…. 1234 upstream mysvr &#123; server 127.0.0.1:7878 weight=1; server 192.168.10.121:3333 weight=2;&#125; ip_hash:nginx会让相同的客户端ip请求相同的服务器。 12345upstream mysvr &#123; server 127.0.0.1:7878; server 192.168.10.121:3333; ip_hash; &#125; 关于nginx负载均衡配置的几个状态参数讲解 down，表示当前的server暂时不参与负载均衡。 backup，预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的压力最轻。 max_fails，允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。 fail_timeout，在经历了max_fails次失败后，暂停服务的时间。max_fails可以和fail_timeout一起使用。 1234upstream mysvr &#123; server 127.0.0.1:7878 weight=2 max_fails=2 fail_timeout=2; server 192.168.10.121:3333 weight=1 max_fails=2 fail_timeout=1; &#125;]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot2.X新特性]]></title>
    <url>%2F2018%2F08%2F28%2FSpringBoot2-X%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[基于 Java 8，支持 Java 9，这意味着不可以使用JDK7 或更旧的版本运行SpringBoot2. 支持 Quartz 调度程序 大大简化了安全自动配置 支持嵌入式 Netty Tomcat, Undertow 和 Jetty 均已支持 HTTP/2 全新的执行器架构，支持 Spring MVC, WebFlux 和 Jersey 使用 Spring WebFlux/WebFlux.fn 提供响应式 Web 编程支持 为各种组件的响应式编程提供了自动化配置，如：Reactive Spring Data、Reactive Spring Security 等 用于响应式 Spring Data Cassandra, MongoDB, Couchbase 和 Redis 的自动化配置和启动器 POM 引入对 Kotlin 1.2.x 的支持，并提供了一个 runApplication 函数，让你通过惯用的 Kotlin 来运行 Spring Boot 应用程序。更多信息请参阅参考文档中对 Kotlin 的支持部分 启动时的 ASCII 图像 Spring Boot banner 现已支持 GIF]]></content>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My Blog]]></title>
    <url>%2F2018%2F08%2F28%2FMy-Blog%2F</url>
    <content type="text"><![CDATA[欢迎来到我的博客世界]]></content>
      <categories>
        <category>Welcome</category>
      </categories>
  </entry>
</search>
